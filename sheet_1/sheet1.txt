Theory:
1.1: No.
1.2: Yes, if you speak in terms of computational hardness it's not more costly. However to train a deep network of course takes up way more ressources than to train a single neuron.
1.3: Yes. Longer training times not so much, but especially overparameterization.
1.4: Yes.

2.1: Yes, they have nicer numerical properties. If they are often used in practice I'm not sure.
2.2: Yes.
2.3: Yes, especially for better generalization ability.
2.4: Yes, they have the ability to reconstruct input from the model's output. There are applications of that for example for probabilistic modeling, generative modeling and representation learning.

3.1: No. They can be, but they are not automatically.
3.2: If "directly" means there is no need for resizing/ cropping/ padding then No. Most of the time an input image has to be resized to the dimensions expected by the model.
3.3: No, it doesn't really learn new classes, it can differentiate between seen ones and unseen inputs though. In the lecture this was especially introduced for one shot learning.
3.4: Yes, but only for later layers!

4.1: Yes.
4.2: No, for detecting human body, hand, facial, and foot keypoints.
4.3: No, the Inception model doesn't have the architecture to process NLP inputs. It's rather for image classification.
4.4: Yes.

5.1: No, convolutions are an integral part of the U-net.
5.2: Yes.
5.3: Yes. Depending on the input size this could be a valid operation, for example like a 1x1 pooling on one dimensional matrix, so a vector.
5.4: No.
